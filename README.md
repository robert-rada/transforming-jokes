# Transforming jokes

Automatic joke generation and evaluation using Transformer models.

## Getting Started

To create the required environments run the following commands:
```
conda env create --name predictor --file=predictor.yml
conda env create --name generator --file=generator.yml
```

The `predictor` environment is used to run all scripts except for `run_generator.py` and `joke_generator.py`.

### Unzip the data

Unzip the `.7z` files in the `data/` folder. 

The three JSON files should be placed in the `data/` folder.

### Download the trained models

All generator models should be in a folder called `generator_models/` (e.g. `generator_models/jokes1`).

Download the trained generator models from the following link:

https://drive.google.com/drive/folders/13kjGxwrfBpNhEfbBgrD7ucvWnlnpzG5u?usp=sharing

All classifier models should be in a folder called `predictor_models/` (e.g. `predictor_models/jokes`).

Download the trained classifier models from the following link:

https://drive.google.com/drive/folders/1qBxOXBg8Z3q4QK2FgIvyxl0dSvsHfK61?usp=sharing

## Reproduce the datasets

### Gathering data from Reddit

This step can be skipped since the repository already contains data gathered from Reddit.

The data was gathered by running `gather_data.py`. Parameters:

```
--subreddit SUBREDDIT   The subreddit from which the data is extracted. Values: {jokes, antijokes, dadjokes}
--limit LIMIT           Number of submissions that will be fetched before stopping
--resume RESUME         Whether to continue adding data to a file generated by a previous run. Values: {True, False}
```

For example, to extract all submissions from the /r/antijokes subreddit, run the following command:

```
python gather_data.py --subreddit=antijokes
```

### Process the data and generate training files

Run `preprocess.py` with the names of the subreddits whose data you want to preprocess as arguments.

To generate all training files run:

```
python preprocess.py jokes dadjokes antijokes
```

## Train the classifier

Run `reddit_predictor.py`. Parameters:

```
--subreddit SUBREDDIT   Name of the dataset used. Values: {jokes, antijokes, dadjokes}
--no_epochs NO_EPOCHS   Number of epochs
--learning_rate LR      Learning rate
--load_model            Continue training from an existing model (whichs should be in the folder predictor_models/SUBREDDIT)
--rebalance             Upsample the minority class to balance the number of positive and negative examples
``` 

The commands used to train the best models:
```
python reddit_predictor.py --subreddit=jokes --no_epochs=10 --learning_rate=0.000001
python reddit_predictor.py --subreddit=dadjokes --no_epochs=10 --learning_rate=0.000001
python reddit_predictor.py --subreddit=antijokes --no_epochs=10 --learning_rate=0.000001
```

To compute the accuracy and other metrics for an existing model run the script with `--no_epochs=0`. For example:

```
python reddit_predictor.py --subreddit=jokes --no_epochs=0 --load_model
```

## Train the generator

The generator was trained on Google Colab with `joke_generator.ipynb`.

To train it locally, use the following commands:

```
conda activate generator
python joke_generator.py jokes
python joke_generator.py dadjokes
python joke_generator.py antijokes
```

## Generate jokes

### Run the generator

Run `run_generator.py`. Parameters:

```
conda activate generator
python run_generator.py RUN_NAME SUBREDDIT NO_SAMPLES (TEMPERATURE)
```

For example, to generate 1000 jokes using the generator trained on data from /r/jokes:

```
conda activate generator
python run_generator.py jokes1 jokes 1000 0.7
```

Files containing generated jokes are placed in the `outputs/` folder.

### Run the predictor

Run `run_predictor.py` to sort jokes by the predicted score and remove reposts.

```
python run_predictor.py jokes
python run_predictor.py dadjokes
python run_predictor.py antijokes
```

The results are written in a file with the prefix `SUBREDDIT_pred` (e.g. `jokes_pred`) in the `outputs/` folder.
